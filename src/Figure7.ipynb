{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bae1a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPolitical News Engagement Forecasting System\\nAuthor: Mohammad Rasoul Salmani\\nProject: AI Final Project\\nDate: 2026/01/22\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Political News Engagement Forecasting System\n",
    "Author: Mohammad Rasoul Salmani\n",
    "Project: AI Final Project\n",
    "Date: 2026/01/22\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5772d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPORTS ====================\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import mean_absolute_error, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ae5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import original model classes from your notebook\n",
    "# You need to copy the model classes from models.ipynb\n",
    "from Models import LSTMForecasterMF, BinnedSeqDataset  # Assuming you save models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters matching the original paper\"\"\"\n",
    "    SEED = 42\n",
    "    BATCH_SIZE = 32\n",
    "    SEQ_LENGTH = 4  # 8 time steps = 2 years (quarterly)\n",
    "    NUM_STANCES = 7  # -3 to +3\n",
    "    NUM_CLUSTERS = 20\n",
    "    DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_PATHS = {\n",
    "        'D1': 'final_models/all_model_data_1.pth',\n",
    "        'D2': 'final_models/all_model_data_2.pth', \n",
    "        'D3': 'final_models/all_model_data_3.pth',\n",
    "        'D4': 'final_models/all_model_data_4.pth'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1ce8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ORIGINAL MODEL LOADER ====================\n",
    "class OriginalModelLoader:\n",
    "    \"\"\"Load and use the original models from the paper\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.models = {}\n",
    "        self.load_all_models()\n",
    "    \n",
    "    def load_all_models(self):\n",
    "        \"\"\"Load all 4 original MFN models\"\"\"\n",
    "        print(\"Loading original models from paper...\")\n",
    "        \n",
    "        for dataset_name, model_path in self.config.MODEL_PATHS.items():\n",
    "            print(f\"  Loading {dataset_name} from {model_path}\")\n",
    "            \n",
    "            try:\n",
    "                # Load model with DataParallel wrapper\n",
    "                model = torch.load(model_path, map_location=self.config.DEVICE)\n",
    "                \n",
    "                # Handle DataParallel models\n",
    "                if isinstance(model, nn.DataParallel):\n",
    "                    model = model.module\n",
    "                \n",
    "                model.eval()\n",
    "                model.to(self.config.DEVICE)\n",
    "                self.models[dataset_name] = model\n",
    "                \n",
    "                print(f\"    ✓ Successfully loaded {dataset_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Failed to load {dataset_name}: {e}\")\n",
    "    \n",
    "    def extract_hidden_states(self, input_data, dataset_name='D1'):\n",
    "        \"\"\"\n",
    "        Extract hidden states from original model\n",
    "        Input should match the original model's expected format\n",
    "        \"\"\"\n",
    "        if dataset_name not in self.models:\n",
    "            raise ValueError(f\"Model for {dataset_name} not loaded\")\n",
    "        \n",
    "        model = self.models[dataset_name]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Prepare input - this needs to match the original model's input format\n",
    "            # Based on LSTMForecasterMF.forward() signature:\n",
    "            # forward(self, count_feats, in_time_feats, out_time_feats, \n",
    "            #         ns_feats, mention_feats, hash_feats, text_feats)\n",
    "            \n",
    "            # Assuming input_data is a dictionary with all required features\n",
    "            hidden_state, predictions = model(\n",
    "                input_data['eng_counts'].to(self.config.DEVICE),\n",
    "                input_data['input_time_feat'].to(self.config.DEVICE),\n",
    "                input_data['output_time_feat'].to(self.config.DEVICE),\n",
    "                input_data['ns_feat'].to(self.config.DEVICE),\n",
    "                input_data['mentions_feat'].to(self.config.DEVICE),\n",
    "                input_data['hashtag_feat'].to(self.config.DEVICE),\n",
    "                input_data['text_feat'].to(self.config.DEVICE)\n",
    "            )\n",
    "            \n",
    "            return hidden_state.cpu().numpy(), predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac28a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATA PROCESSOR ====================\n",
    "class DataProcessor:\n",
    "    \"\"\"Process raw JSON data into sequences compatible with original models\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        np.random.seed(config.SEED)\n",
    "        torch.manual_seed(config.SEED)\n",
    "    \n",
    "    def load_data(self, filepath, sample_size=None):\n",
    "        \"\"\"Load JSON data and convert to DataFrame\"\"\"\n",
    "        print(f\"Loading data from {filepath}...\")\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to list of records\n",
    "        records = []\n",
    "        items_processed = 0\n",
    "        \n",
    "        for key, value in data.items():\n",
    "            if sample_size and items_processed >= sample_size:\n",
    "                break\n",
    "                \n",
    "            records.append({\n",
    "                'user_id': value['user_id_anonymized'],\n",
    "                'timestamp': pd.to_datetime(value['created_at']),\n",
    "                'sources': value['news sources'],\n",
    "                'stances': value['partisan stance']\n",
    "            })\n",
    "            items_processed += 1\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        print(f\"Loaded {len(df)} records from {items_processed} users\")\n",
    "        return df\n",
    "    \n",
    "    def prepare_for_original_model(self, df):\n",
    "        \"\"\"\n",
    "        Prepare data in the exact format expected by the original model\n",
    "        This is a simplified version - you may need to adjust based on actual data format\n",
    "        \"\"\"\n",
    "        print(\"Preparing data for original model...\")\n",
    "        \n",
    "        # Group by user and create sequences\n",
    "        user_sequences = defaultdict(list)\n",
    "        \n",
    "        # For each user, create 8 time steps of data\n",
    "        for user_id, user_df in tqdm(df.groupby('user_id'), desc=\"Processing users\"):\n",
    "            user_df = user_df.sort_values('timestamp')\n",
    "            \n",
    "            # Create quarterly aggregates (simplified)\n",
    "            user_df['quarter'] = user_df['timestamp'].dt.to_period('Q')\n",
    "            quarterly_data = []\n",
    "            \n",
    "            for quarter, quarter_df in user_df.groupby('quarter'):\n",
    "                # Count engagements per stance\n",
    "                stance_counts = [0] * 7\n",
    "                for stances in quarter_df['stances']:\n",
    "                    for stance in stances:\n",
    "                        idx = int(stance) + 3\n",
    "                        if 0 <= idx < 7:\n",
    "                            stance_counts[idx] += 1\n",
    "                quarterly_data.append(stance_counts)\n",
    "            \n",
    "            # If we have enough quarters, create sequences\n",
    "            if len(quarterly_data) >= 9:  # 8 input + 1 output\n",
    "                # Create multiple overlapping sequences\n",
    "                for i in range(len(quarterly_data) - 8):\n",
    "                    input_seq = quarterly_data[i:i+8]\n",
    "                    output_seq = quarterly_data[i+8]\n",
    "                    \n",
    "                    # Create feature dictionary matching original format\n",
    "                    features = {\n",
    "                        'eng_counts': np.array(input_seq).reshape(1, 8, 7),\n",
    "                        'label_original': np.array(output_seq).reshape(1, 7),\n",
    "                        # Add dummy features for other inputs (simplified)\n",
    "                        'input_time_feat': np.zeros((1, 8, 4)),\n",
    "                        'output_time_feat': np.zeros((1, 4)),\n",
    "                        'ns_feat': np.zeros((1, 1536)),\n",
    "                        'mentions_feat': np.zeros((1, 1536)),\n",
    "                        'hashtag_feat': np.zeros((1, 1536)),\n",
    "                        'text_feat': np.zeros((1, 3072))\n",
    "                    }\n",
    "                    \n",
    "                    user_sequences[user_id].append(features)\n",
    "        \n",
    "        # Flatten all sequences\n",
    "        all_sequences = []\n",
    "        for user_seqs in user_sequences.values():\n",
    "            all_sequences.extend(user_seqs)\n",
    "        \n",
    "        print(f\"Created {len(all_sequences)} sequences\")\n",
    "        return all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25dc472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CLUSTERING ANALYZER ====================\n",
    "class UserClusterAnalyzer:\n",
    "    \"\"\"Analyze user behavior patterns through clustering\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def cluster_users(self, representations, n_clusters=20):\n",
    "        \"\"\"Cluster users based on their representations\"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=self.config.SEED, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(representations)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        if len(np.unique(cluster_labels)) > 1:\n",
    "            silhouette_avg = silhouette_score(representations, cluster_labels)\n",
    "            print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "        else:\n",
    "            print(\"Only one cluster found, silhouette score not calculated\")\n",
    "        \n",
    "        return cluster_labels, kmeans.cluster_centers_\n",
    "    \n",
    "    def analyze_cluster_topics(self, user_texts, cluster_labels, top_n=10):\n",
    "        \"\"\"Find distinguishing terms for each cluster\"\"\"\n",
    "        if not user_texts:\n",
    "            return {}\n",
    "        \n",
    "        vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "        X = vectorizer.fit_transform(user_texts)\n",
    "        \n",
    "        cluster_terms = {}\n",
    "        for cluster_id in np.unique(cluster_labels):\n",
    "            cluster_mask = cluster_labels == cluster_id\n",
    "            other_mask = ~cluster_mask\n",
    "            \n",
    "            if sum(cluster_mask) < 2 or sum(other_mask) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Get feature frequencies\n",
    "            cluster_counts = X[cluster_mask].sum(axis=0).A1\n",
    "            other_counts = X[other_mask].sum(axis=0).A1\n",
    "            \n",
    "            # Simple frequency ratio\n",
    "            total_cluster = cluster_counts.sum()\n",
    "            total_other = other_counts.sum()\n",
    "            \n",
    "            if total_cluster == 0 or total_other == 0:\n",
    "                continue\n",
    "            \n",
    "            term_scores = []\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            for i, term in enumerate(feature_names):\n",
    "                cluster_freq = cluster_counts[i] / total_cluster if total_cluster > 0 else 0\n",
    "                other_freq = other_counts[i] / total_other if total_other > 0 else 0\n",
    "                \n",
    "                if cluster_freq > 0:\n",
    "                    score = cluster_freq / (other_freq + 1e-10)  # Avoid division by zero\n",
    "                    term_scores.append((term, score, cluster_freq))\n",
    "            \n",
    "            # Sort by score\n",
    "            term_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            cluster_terms[cluster_id] = term_scores[:top_n]\n",
    "        \n",
    "        return cluster_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38eb1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION ====================\n",
    "class Visualization:\n",
    "    \"\"\"Create visualizations for the project\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_simple_heatmap(cluster_data, save_path='cluster_heatmap.png'):\n",
    "        \"\"\"\n",
    "        Simple heatmap showing engagement patterns per cluster\n",
    "        cluster_data: dictionary with cluster_id as key and dict as value\n",
    "        \"\"\"\n",
    "        if not cluster_data:\n",
    "            print(\"No cluster data to visualize\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data for heatmap\n",
    "        n_clusters = len(cluster_data)\n",
    "        engagement_matrix = np.zeros((n_clusters, 7))\n",
    "        cluster_sizes = []\n",
    "        cluster_labels = []\n",
    "        \n",
    "        for cluster_id, data in sorted(cluster_data.items()):\n",
    "            engagement_matrix[cluster_id] = data['engagement_pattern']\n",
    "            cluster_sizes.append(data['size'])\n",
    "            cluster_labels.append(f\"Cluster {cluster_id}\\n(n={data['size']})\")\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        ax = sns.heatmap(engagement_matrix, \n",
    "                        annot=True, \n",
    "                        fmt='.1f',\n",
    "                        cmap='YlOrRd',\n",
    "                        xticklabels=['-3', '-2', '-1', '0', '+1', '+2', '+3'],\n",
    "                        yticklabels=cluster_labels,\n",
    "                        cbar_kws={'label': 'Average Engagement Count'})\n",
    "        \n",
    "        plt.title('Average News Engagement by Cluster and Political Stance', fontsize=16, pad=20)\n",
    "        plt.xlabel('Political Stance', fontsize=14)\n",
    "        plt.ylabel('Cluster (with user count)', fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=10, rotation=0)\n",
    "        \n",
    "        # Add cluster sizes as text on the right\n",
    "        for i, size in enumerate(cluster_sizes):\n",
    "            plt.text(engagement_matrix.shape[1] + 0.5, i + 0.5, \n",
    "                    f\"Size: {size}\", \n",
    "                    ha='left', va='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Heatmap saved to {save_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_cluster_sizes(cluster_labels, save_path='cluster_sizes.png'):\n",
    "        \"\"\"Plot bar chart of cluster sizes\"\"\"\n",
    "        cluster_sizes = Counter(cluster_labels)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        clusters = sorted(cluster_sizes.keys())\n",
    "        sizes = [cluster_sizes[c] for c in clusters]\n",
    "        \n",
    "        bars = plt.bar(range(len(clusters)), sizes, color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar, size in zip(bars, sizes):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(size), ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.xlabel('Cluster ID', fontsize=12)\n",
    "        plt.ylabel('Number of Users', fontsize=12)\n",
    "        plt.title(f'Distribution of Users Across {len(clusters)} Clusters', fontsize=14)\n",
    "        plt.xticks(range(len(clusters)), [f'Cluster {c}' for c in clusters], rotation=45)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_engagement_trends(cluster_data, save_path='engagement_trends.png'):\n",
    "        \"\"\"Plot engagement trends over time for each cluster\"\"\"\n",
    "        fig, axes = plt.subplots(5, 4, figsize=(20, 15))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, (cluster_id, data) in enumerate(sorted(cluster_data.items())):\n",
    "            if idx >= len(axes):\n",
    "                break\n",
    "            \n",
    "            engagement_pattern = data['engagement_pattern']\n",
    "            \n",
    "            # Plot each stance as a line\n",
    "            stances = ['-3', '-2', '-1', '0', '+1', '+2', '+3']\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, 7))\n",
    "            \n",
    "            for stance_idx in range(7):\n",
    "                axes[idx].plot(engagement_pattern[:, stance_idx], \n",
    "                             label=stances[stance_idx], \n",
    "                             color=colors[stance_idx],\n",
    "                             linewidth=2,\n",
    "                             marker='o')\n",
    "            \n",
    "            axes[idx].set_title(f'Cluster {cluster_id} (n={data[\"size\"]})', fontsize=11)\n",
    "            axes[idx].set_xlabel('Time Step', fontsize=9)\n",
    "            axes[idx].set_ylabel('Engagement', fontsize=9)\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "            axes[idx].legend(loc='upper right', fontsize=7)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(len(cluster_data), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Engagement Trends Across Clusters', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61309465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MAIN PIPELINE WITH ORIGINAL MODELS ====================\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline using original models\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"POLITICAL NEWS ENGAGEMENT FORECASTING SYSTEM - ORIGINAL MODELS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    print(f\"Device: {config.DEVICE}\")\n",
    "    print(f\"Seed: {config.SEED}\")\n",
    "    \n",
    "    # Step 1: Load original models\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"STEP 1: LOADING ORIGINAL MODELS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model_loader = OriginalModelLoader(config)\n",
    "    \n",
    "    if not model_loader.models:\n",
    "        print(\"ERROR: No models loaded. Check model paths.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"✓ Loaded {len(model_loader.models)} original models\")\n",
    "    \n",
    "    # Step 2: Load and prepare data\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"STEP 2: LOADING AND PREPARING DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    data_processor = DataProcessor(config)\n",
    "    \n",
    "    # Load sample data\n",
    "    df = data_processor.load_data('data/icwsm-2024-forecasting-data-anon.json', \n",
    "                                 sample_size=20000)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"ERROR: No data loaded!\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nData statistics:\")\n",
    "    print(f\"- Total records: {len(df):,}\")\n",
    "    print(f\"- Unique users: {df['user_id'].nunique():,}\")\n",
    "    print(f\"- Time range: {df['timestamp'].min().date()} to {df['timestamp'].max().date()}\")\n",
    "    \n",
    "    # Prepare data for model\n",
    "    all_sequences = data_processor.prepare_for_original_model(df)\n",
    "    \n",
    "    if len(all_sequences) == 0:\n",
    "        print(\"ERROR: No sequences created!\")\n",
    "        print(\"Users may not have enough data points (need at least 9 quarters)\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\n✓ Created {len(all_sequences):,} sequences\")\n",
    "    \n",
    "    # Step 3: Extract hidden states using original model\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"STEP 3: EXTRACTING HIDDEN STATES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # We'll use the first model (D1) for extraction\n",
    "    # In practice, you might want to use all models based on time period\n",
    "    print(\"Using model D1 for feature extraction...\")\n",
    "    \n",
    "    # Extract features for a subset of sequences (for speed)\n",
    "    sample_size = min(1000, len(all_sequences))\n",
    "    print(f\"Processing {sample_size} sequences...\")\n",
    "    \n",
    "    all_hidden_states = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(sample_size), desc=\"Extracting features\"):\n",
    "            seq_data = all_sequences[i]\n",
    "            \n",
    "            # Convert numpy arrays to torch tensors\n",
    "            input_data = {\n",
    "                'eng_counts': torch.FloatTensor(seq_data['eng_counts']),\n",
    "                'input_time_feat': torch.FloatTensor(seq_data['input_time_feat']),\n",
    "                'output_time_feat': torch.FloatTensor(seq_data['output_time_feat']),\n",
    "                'ns_feat': torch.FloatTensor(seq_data['ns_feat']),\n",
    "                'mentions_feat': torch.FloatTensor(seq_data['mentions_feat']),\n",
    "                'hashtag_feat': torch.FloatTensor(seq_data['hashtag_feat']),\n",
    "                'text_feat': torch.FloatTensor(seq_data['text_feat'])\n",
    "            }\n",
    "            \n",
    "            # Extract hidden states\n",
    "            hidden_state, prediction = model_loader.extract_hidden_states(input_data, 'D1')\n",
    "            all_hidden_states.append(hidden_state.flatten())\n",
    "            all_predictions.append(prediction.flatten())\n",
    "            all_labels.append(seq_data['label_original'].flatten())\n",
    "    \n",
    "    user_repr = np.array(all_hidden_states)\n",
    "    predictions = np.array(all_predictions)\n",
    "    true_labels = np.array(all_labels)\n",
    "    \n",
    "    print(f\"\\n✓ Extracted features for {user_repr.shape[0]} sequences\")\n",
    "    print(f\"  Feature dimension: {user_repr.shape[1]}\")\n",
    "    \n",
    "    # Step 4: Clustering analysis\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"STEP 4: CLUSTERING ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    analyzer = UserClusterAnalyzer(config)\n",
    "    cluster_labels, cluster_centers = analyzer.cluster_users(user_repr, config.NUM_CLUSTERS)\n",
    "    \n",
    "    # Analyze cluster sizes\n",
    "    cluster_sizes = Counter(cluster_labels)\n",
    "    print(f\"\\nCluster distribution:\")\n",
    "    for cluster_id in sorted(cluster_sizes.keys()):\n",
    "        size = cluster_sizes[cluster_id]\n",
    "        percentage = (size / len(cluster_labels)) * 100\n",
    "        print(f\"  Cluster {cluster_id:2d}: {size:3d} users ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Step 5: Create cluster visualizations\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"STEP 5: CREATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # تابع کمکی برای محاسبه میانگین گرایش\n",
    "    def calculate_avg_stance(engagement_pattern):\n",
    "        \"\"\"محاسبه میانگین وزنی گرایش سیاسی\"\"\"\n",
    "        stances = np.array([-3, -2, -1, 0, 1, 2, 3])\n",
    "        total_engagements = np.sum(engagement_pattern)\n",
    "        if total_engagements > 0:\n",
    "            weighted_avg = np.sum(stances * engagement_pattern) / total_engagements\n",
    "            return weighted_avg\n",
    "        return 0\n",
    "    \n",
    "    # Prepare cluster data for visualization\n",
    "    cluster_data = {}\n",
    "    for cluster_id in range(config.NUM_CLUSTERS):\n",
    "        cluster_users_idx = np.where(cluster_labels == cluster_id)[0]\n",
    "        \n",
    "        if len(cluster_users_idx) > 0:\n",
    "            # Calculate average engagement pattern for this cluster\n",
    "            cluster_engagements = true_labels[cluster_users_idx]\n",
    "            avg_engagement = np.mean(cluster_engagements, axis=0)\n",
    "            \n",
    "            cluster_data[cluster_id] = {\n",
    "                'size': len(cluster_users_idx),\n",
    "                'avg_stance': calculate_avg_stance(avg_engagement),\n",
    "                'engagement_pattern': avg_engagement,\n",
    "                'user_indices': cluster_users_idx\n",
    "            }\n",
    "    \n",
    "    # Create visualizations\n",
    "    viz = Visualization()\n",
    "    \n",
    "    # 1. Cluster sizes bar chart\n",
    "    print(\"\\n1. Creating cluster sizes visualization...\")\n",
    "    viz.plot_cluster_sizes(cluster_labels, 'results/cluster_sizes.png')\n",
    "    \n",
    "    # 2. Simple heatmap of engagement patterns\n",
    "    print(\"\\n2. Creating engagement heatmap...\")\n",
    "    viz.plot_simple_heatmap(cluster_data, 'results/cluster_heatmap.png')\n",
    "    \n",
    "    # 3. Engagement trends (if we have time-series data)\n",
    "    print(\"\\n3. Creating engagement trends visualization...\")\n",
    "    # This would need time-series engagement data\n",
    "    \n",
    "    # Step 6: Performance evaluation\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"STEP 6: MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate MAE for each stance\n",
    "    mae_per_stance = []\n",
    "    for stance_idx in range(7):\n",
    "        mae = mean_absolute_error(true_labels[:, stance_idx], predictions[:, stance_idx])\n",
    "        mae_per_stance.append(mae)\n",
    "    \n",
    "    print(\"\\nModel Performance (MAE per stance):\")\n",
    "    print(\"-\" * 40)\n",
    "    for idx, (stance, mae) in enumerate(zip(['-3', '-2', '-1', '0', '+1', '+2', '+3'], mae_per_stance)):\n",
    "        print(f\"  Stance {stance}: {mae:.4f}\")\n",
    "    \n",
    "    avg_mae = np.mean(mae_per_stance)\n",
    "    print(f\"\\n  Average MAE: {avg_mae:.4f}\")\n",
    "    \n",
    "    # Step 7: Save results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"STEP 7: SAVING RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    results = {\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'cluster_data': cluster_data,\n",
    "        'user_representations': user_repr,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'mae_per_stance': mae_per_stance,\n",
    "        'avg_mae': avg_mae\n",
    "    }\n",
    "    \n",
    "    with open('results/clustering_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(\"✓ Results saved to 'results/' directory:\")\n",
    "    print(\"  - clustering_results.pkl\")\n",
    "    print(\"  - cluster_sizes.png\")\n",
    "    print(\"  - cluster_heatmap.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return cluster_labels, cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3511156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POLITICAL NEWS ENGAGEMENT FORECASTING SYSTEM - ORIGINAL MODELS\n",
      "======================================================================\n",
      "Device: cpu\n",
      "Seed: 42\n",
      "\n",
      "==================================================\n",
      "STEP 1: LOADING ORIGINAL MODELS\n",
      "==================================================\n",
      "Loading original models from paper...\n",
      "  Loading D1 from final_models/all_model_data_1.pth\n",
      "    ✗ Failed to load D1: [Errno 2] No such file or directory: 'final_models/all_model_data_1.pth'\n",
      "  Loading D2 from final_models/all_model_data_2.pth\n",
      "    ✗ Failed to load D2: [Errno 2] No such file or directory: 'final_models/all_model_data_2.pth'\n",
      "  Loading D3 from final_models/all_model_data_3.pth\n",
      "    ✗ Failed to load D3: [Errno 2] No such file or directory: 'final_models/all_model_data_3.pth'\n",
      "  Loading D4 from final_models/all_model_data_4.pth\n",
      "    ✗ Failed to load D4: [Errno 2] No such file or directory: 'final_models/all_model_data_4.pth'\n",
      "ERROR: No models loaded. Check model paths.\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# ==================== EXECUTION ====================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Run the main pipeline\n",
    "        cluster_labels, cluster_data = main()\n",
    "        \n",
    "        if cluster_labels is not None:\n",
    "            print(\"\\nSummary:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Total users clustered: {len(cluster_labels)}\")\n",
    "            print(f\"Number of clusters: {len(np.unique(cluster_labels))}\")\n",
    "            print(f\"Results saved in 'results/' directory\")\n",
    "            \n",
    "            # Show sample cluster\n",
    "            if cluster_data:\n",
    "                sample_cluster = list(cluster_data.keys())[0]\n",
    "                print(f\"\\nSample cluster #{sample_cluster}:\")\n",
    "                print(f\"  Size: {cluster_data[sample_cluster]['size']} users\")\n",
    "                print(f\"  Avg stance: {cluster_data[sample_cluster]['avg_stance']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fa9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
