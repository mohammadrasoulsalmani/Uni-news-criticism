{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ce018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c700eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 1: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ùˆ Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3d9867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø¬Ø§Ø±ÛŒ: e:\\AI_Proj\\Uni\\icwsm-2024-news-criticism\\src\n",
      "Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡: e:\\AI_Proj\\Uni\\icwsm-2024-news-criticism\\src\\data\n"
     ]
    }
   ],
   "source": [
    "# ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ±Ù‡Ø§\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, 'data')\n",
    "print(f\"Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø¬Ø§Ø±ÛŒ: {current_dir}\")\n",
    "print(f\"Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡: {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c721ae95",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'e:\\\\AI_Proj\\\\Uni\\\\icwsm-2024-news-criticism\\\\src\\\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m files = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ data:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'e:\\\\AI_Proj\\\\Uni\\\\icwsm-2024-news-criticism\\\\src\\\\data'"
     ]
    }
   ],
   "source": [
    "# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡\n",
    "files = os.listdir(data_dir)\n",
    "print(\"\\nÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ data:\")\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    file_size = os.path.getsize(file_path) / (1024*1024)  # Ø­Ø¬Ù… Ø¨Ù‡ Ù…Ú¯Ø§Ø¨Ø§ÛŒØª\n",
    "    print(f\"  - {file} ({file_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c631bee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     torch.backends.cudnn.benchmark = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     10\u001b[39m     torch.backends.cudnn.deterministic = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mset_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_seed\u001b[39m(seed=\u001b[32m42\u001b[39m):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"ØªÙ†Ø¸ÛŒÙ… seed Ø¨Ø±Ø§ÛŒ reproducibility Ø¯Ø± ØªÙ…Ø§Ù… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtorch\u001b[49m.manual_seed(seed)\n\u001b[32m      5\u001b[39m     torch.cuda.manual_seed(seed)\n\u001b[32m      6\u001b[39m     torch.cuda.manual_seed_all(seed)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ… seed Ø¨Ø±Ø§ÛŒ reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"ØªÙ†Ø¸ÛŒÙ… seed Ø¨Ø±Ø§ÛŒ reproducibility Ø¯Ø± ØªÙ…Ø§Ù… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54048477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±ÛŒ:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ø¨Ø±Ø±Ø³ÛŒ GPU\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mØ§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±ÛŒ:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtorch\u001b[49m.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCUDA available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.is_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Ø¨Ø±Ø±Ø³ÛŒ GPU\n",
    "print(f\"\\nØ§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±ÛŒ:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Ø§Ø² CPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865c9949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 2: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ\n",
      "============================================================\n",
      "\n",
      "1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ (news_df.csv)...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ: [Errno 2] No such file or directory: 'e:\\\\AI_Proj\\\\Uni\\\\icwsm-2024-news-criticism\\\\src\\\\data\\\\news_df.csv'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 2: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 2: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2.1 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ\n",
    "print(\"\\n1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ (news_df.csv)...\")\n",
    "news_df_path = os.path.join(data_dir, 'news_df.csv')\n",
    "try:\n",
    "    news_df = pd.read_csv(news_df_path)\n",
    "    print(f\"   âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {news_df.shape} Ø³Ø·Ø± Ùˆ {news_df.shape[1]} Ø³ØªÙˆÙ†\")\n",
    "    print(f\"   ğŸ“Š Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(news_df.columns)}\")\n",
    "    \n",
    "    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ\n",
    "    print(f\"\\n   Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ:\")\n",
    "    print(f\"   ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ: {len(news_df)}\")\n",
    "    print(f\"   Ø±Ù†Ø¬ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø­Ø²Ø¨ÛŒ: {news_df['Partisan Score'].min()} ØªØ§ {news_df['Partisan Score'].max()}\")\n",
    "    \n",
    "    # Ù†Ù…Ø§ÛŒØ´ 5 Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ø§ÙˆÙ„\n",
    "    print(f\"\\n   Ø§ÙˆÙ„ÛŒÙ† 5 Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ:\")\n",
    "    for i in range(min(5, len(news_df))):\n",
    "        print(f\"   {i+1}. {news_df.iloc[i]['Source']} - Ø§Ù…ØªÛŒØ§Ø²: {news_df.iloc[i]['Partisan Score']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ: {e}\")\n",
    "    news_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c2cf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ (annotated_data_anonymized.jsonl)...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡: [Errno 2] No such file or directory: 'e:\\\\AI_Proj\\\\Uni\\\\icwsm-2024-news-criticism\\\\src\\\\data\\\\annotated_data_anonymized.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡\n",
    "print(\"\\n2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ (annotated_data_anonymized.jsonl)...\")\n",
    "annotated_path = os.path.join(data_dir, 'annotated_data_anonymized.jsonl')\n",
    "try:\n",
    "    # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ JSONL\n",
    "    annotated_data = []\n",
    "    with open(annotated_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            annotated_data.append(json.loads(line))\n",
    "    \n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame\n",
    "    annotated_df = pd.DataFrame(annotated_data)\n",
    "    print(f\"   âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {annotated_df.shape} Ø³Ø·Ø±\")\n",
    "    print(f\"   ğŸ“Š Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(annotated_df.columns)}\")\n",
    "    \n",
    "    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "    print(f\"\\n   Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡:\")\n",
    "    print(f\"   - ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§: {len(annotated_df)}\")\n",
    "    if 'label' in annotated_df.columns:\n",
    "        print(f\"   - ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§:\")\n",
    "        label_counts = annotated_df['label'].value_counts()\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"     * Ø¨Ø±Ú†Ø³Ø¨ {label}: {count} ({count/len(annotated_df)*100:.1f}%)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡: {e}\")\n",
    "    annotated_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "816aa9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ (data_for_analysis_anonymized.jsonl)...\n",
      "   Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ù„Ø­Ø¸Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„: [Errno 2] No such file or directory: 'e:\\\\AI_Proj\\\\Uni\\\\icwsm-2024-news-criticism\\\\src\\\\data\\\\data_for_analysis_anonymized.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„\n",
    "print(\"\\n3. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ (data_for_analysis_anonymized.jsonl)...\")\n",
    "analysis_path = os.path.join(data_dir, 'data_for_analysis_anonymized.jsonl')\n",
    "try:\n",
    "    # Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…\n",
    "    print(\"   Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ù„Ø­Ø¸Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)...\")\n",
    "    \n",
    "    # Ø§Ø¨ØªØ¯Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·ÙˆØ· Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù…Ø§Ø±ÛŒÙ…\n",
    "    with open(analysis_path, 'r', encoding='utf-8') as f:\n",
    "        num_lines = sum(1 for _ in f)\n",
    "    print(f\"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø®Ø·ÙˆØ· Ø¯Ø± ÙØ§ÛŒÙ„: {num_lines:,}\")\n",
    "    \n",
    "    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø§ÙˆÙ„ÛŒÙ† 1000 Ø®Ø·)\n",
    "    analysis_data = []\n",
    "    with open(analysis_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 1000:  # ÙÙ‚Ø· 1000 Ø®Ø· Ø§ÙˆÙ„\n",
    "                analysis_data.append(json.loads(line))\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame\n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    print(f\"   âœ… Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {analysis_df.shape} Ø³Ø·Ø±\")\n",
    "    print(f\"   ğŸ“Š Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(analysis_df.columns)}\")\n",
    "    \n",
    "    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±ÛŒ\n",
    "    print(f\"\\n   Ø¢Ù…Ø§Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:\")\n",
    "    print(f\"   - ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ù…Ù†Ø­ØµØ±Ø¨ÙØ±Ø¯: {analysis_df['user'].nunique() if 'user' in analysis_df.columns else 'N/A'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„: {e}\")\n",
    "    analysis_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d37be847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 3: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 3: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 3: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ù„ÛŒØ³Øª Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
    "modules = [\n",
    "    'preprocessing_utils',\n",
    "    'feature_extractors', \n",
    "    'analysis_utils',\n",
    "    'torch_datasets',\n",
    "    'models',\n",
    "    'model_utils',\n",
    "    'labelling_functions',\n",
    "    'inference_utils',\n",
    "    'plot_utils'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5b9e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ preprocessing_utils...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ preprocessing_utils: cannot import name 'PS_LF' from partially initialized module 'labelling_functions' (most likely due ...\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ feature_extractors...\n",
      "   âœ… feature_extractors Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ analysis_utils...\n",
      "   âœ… analysis_utils Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ torch_datasets...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ torch_datasets: cannot import name 'translation_init' from partially initialized module 'preprocessing_utils' (most ...\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ models...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ models: No module named 'transformers'...\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ model_utils...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ model_utils: No module named 'sklearn'...\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ labelling_functions...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ labelling_functions: cannot import name 'translation_init' from partially initialized module 'preprocessing_utils' (most ...\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ inference_utils...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ inference_utils: No module named 'torch'...\n",
      "   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ plot_utils...\n",
      "   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ plot_utils: No module named 'torch'...\n",
      "\n",
      "Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§:\n",
      "   âŒ preprocessing_utils\n",
      "   âœ… feature_extractors\n",
      "   âœ… analysis_utils\n",
      "   âŒ torch_datasets\n",
      "   âŒ models\n",
      "   âŒ model_utils\n",
      "   âŒ labelling_functions\n",
      "   âŒ inference_utils\n",
      "   âŒ plot_utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:754: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<string>:756: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<string>:758: SyntaxWarning: invalid escape sequence '\\p'\n"
     ]
    }
   ],
   "source": [
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§\n",
    "loaded_modules = {}\n",
    "for module_name in modules:\n",
    "    try:\n",
    "        # Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„\n",
    "        module_file = f\"{module_name}.py\"\n",
    "        \n",
    "        if os.path.exists(module_file):\n",
    "            print(f\"   ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {module_name}...\")\n",
    "            \n",
    "            # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„\n",
    "            with open(module_file, 'r', encoding='utf-8') as f:\n",
    "                module_content = f.read()\n",
    "            \n",
    "            # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø§Ú˜ÙˆÙ„\n",
    "            exec(module_content, globals())\n",
    "            loaded_modules[module_name] = True\n",
    "            print(f\"   âœ… {module_name} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ ÙØ§ÛŒÙ„ {module_file} ÛŒØ§ÙØª Ù†Ø´Ø¯\")\n",
    "            loaded_modules[module_name] = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {module_name}: {str(e)[:100]}...\")\n",
    "        loaded_modules[module_name] = False\n",
    "\n",
    "print(f\"\\nÙ†ØªØ§ÛŒØ¬ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§:\")\n",
    "for module_name, status in loaded_modules.items():\n",
    "    status_symbol = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"   {status_symbol} {module_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51eef665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 4: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
      "============================================================\n",
      "âš ï¸ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 4: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 4: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if loaded_modules['preprocessing_utils'] and analysis_df is not None:\n",
    "    print(\"Ø§Ù†Ø¬Ø§Ù… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ...\")\n",
    "    \n",
    "    try:\n",
    "        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´\n",
    "        # Ø§Ø¨ØªØ¯Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
    "        print(f\"\\nØ³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ:\")\n",
    "        print(f\"- Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {list(analysis_df.columns)}\")\n",
    "        print(f\"- ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡: {len(analysis_df)}\")\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ\n",
    "        required_columns = ['text', 'user', 'tweet_id', 'matched_partisans', 'matched_sources']\n",
    "        missing_columns = [col for col in required_columns if col not in analysis_df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"âš ï¸ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯: {missing_columns}\")\n",
    "            print(\"ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯...\")\n",
    "            \n",
    "            # Ø§Ú¯Ø± Ø³ØªÙˆÙ† matched_partisans ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ ÛŒÚ© Ø³ØªÙˆÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
    "            if 'matched_partisans' not in analysis_df.columns:\n",
    "                print(\"Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ† matched_partisans Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "                analysis_df['matched_partisans'] = np.random.choice([-2, -3, 2, 3], len(analysis_df))\n",
    "            \n",
    "            # Ø§Ú¯Ø± Ø³ØªÙˆÙ† matched_sources ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ ÛŒÚ© Ø³ØªÙˆÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
    "            if 'matched_sources' not in analysis_df.columns:\n",
    "                print(\"Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ† matched_sources Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "                analysis_df['matched_sources'] = [['CNN'] if i % 2 == 0 else ['FoxNews'] for i in range(len(analysis_df))]\n",
    "        \n",
    "        # Ø§ÙØ²ÙˆØ¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯\n",
    "        if 'tweet_type' not in analysis_df.columns:\n",
    "            analysis_df['tweet_type'] = 'replied_to'\n",
    "        \n",
    "        if 'referenced_text' not in analysis_df.columns:\n",
    "            analysis_df['referenced_text'] = [[] for _ in range(len(analysis_df))]\n",
    "        \n",
    "        if 'matched_mentions' not in analysis_df.columns:\n",
    "            analysis_df['matched_mentions'] = analysis_df['matched_sources']\n",
    "        \n",
    "        if 'matched_urls' not in analysis_df.columns:\n",
    "            analysis_df['matched_urls'] = [[] for _ in range(len(analysis_df))]\n",
    "        \n",
    "        if 'tweet_public_metrics' not in analysis_df.columns:\n",
    "            analysis_df['tweet_public_metrics'] = [{'retweet_count': 0, 'reply_count': 1, 'like_count': 7, 'quote_count': 0} \n",
    "                                                  for _ in range(len(analysis_df))]\n",
    "        \n",
    "        # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ (Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª)\n",
    "        sample_size = min(500, len(analysis_df))\n",
    "        sample_data = analysis_df.head(sample_size).copy()\n",
    "        \n",
    "        print(f\"\\nÙ¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ {sample_size} Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\")\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ø´Ù†Ø§Ø³Ø§Ú¯Ø± Ø±ÛŒØªÙˆÛŒÛŒØª Ù…Ø³ØªÙ‚ÛŒÙ…\n",
    "        if news_df is not None:\n",
    "            drt_identifier = DRIdentifier(news_df)\n",
    "            \n",
    "            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø±ÛŒØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…\n",
    "            sample_data = identify_direct_retweets(sample_data, drt_identifier)\n",
    "            print(f\"   - Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø±ÛŒØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯\")\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªÙˆÛŒÛŒØªØ±\n",
    "            twitter_handles = news_df.explode(\"Twitter Handle\")\n",
    "            twitter_handles = twitter_handles.loc[twitter_handles['Twitter Handle'].notna()]\n",
    "            twitter_handles = twitter_handles[\"Twitter Handle\"].tolist()\n",
    "            \n",
    "            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø·ÙˆÙ„Ø§Ù†ÛŒ\n",
    "            sample_data = identify_long_replychains(sample_data, twitter_handles)\n",
    "            print(f\"   - Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯\")\n",
    "        \n",
    "        # Ø­Ø°Ù ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† mention\n",
    "        sample_data = remove_non_mentions(sample_data)\n",
    "        print(f\"   - Ø­Ø°Ù ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† mention Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯\")\n",
    "        \n",
    "        # Ø¯ÙˆØ®ØªÙ† Ù…ØªÙ† ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§\n",
    "        sample_data[\"stitched_text\"] = sample_data.apply(lambda x: stitch_tweets(x), axis=1)\n",
    "        print(f\"   - Ø¯ÙˆØ®ØªÙ† Ù…ØªÙ† ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯\")\n",
    "        \n",
    "        print(f\"\\nâœ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯\")\n",
    "        print(f\"   - Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {sample_data.shape}\")\n",
    "        print(f\"   - ØªØ¹Ø¯Ø§Ø¯ Ø±ÛŒØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…: {sample_data['direct_retweet'].sum() if 'direct_retweet' in sample_data.columns else 0}\")\n",
    "        \n",
    "        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡\n",
    "        preprocessed_path = os.path.join(current_dir, 'preprocessed_sample.pkl')\n",
    "        sample_data.to_pickle(preprocessed_path)\n",
    "        print(f\"   - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¯Ø± {preprocessed_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯\")\n",
    "        \n",
    "        preprocessed_data = sample_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}\")\n",
    "        preprocessed_data = None\n",
    "else:\n",
    "    print(\"âš ï¸ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")\n",
    "    preprocessed_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa27cb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 5: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
      "============================================================\n",
      "âš ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 5: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 5: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if loaded_modules['feature_extractors'] and preprocessed_data is not None and news_df is not None:\n",
    "    print(\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡...\")\n",
    "    \n",
    "    try:\n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ§Ø³ØªÙ…Ø¯Ø§Ø±Ø§Ù† Ù†Ù…ÙˆÙ†Ù‡\n",
    "        pol_data = {\n",
    "            'user_name': ['politician1', 'politician2', 'politician3', 'politician4'],\n",
    "            'party_name': ['Republican', 'Democrat', 'Republican', 'Democrat']\n",
    "        }\n",
    "        pol_df = pd.DataFrame(pol_data)\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ extractor\n",
    "        print(\"Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒÚ©Ù†Ù†Ø¯Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±...\")\n",
    "        user_feats_extractor = ExtractUserFeats(\n",
    "            news_df=news_df,\n",
    "            pol_df=pol_df,\n",
    "            following_path=data_dir  # Ù…Ø³ÛŒØ± Ù…ÙˆÙ‚Øª\n",
    "        )\n",
    "        \n",
    "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø§Ø±Ø¨Ø±\n",
    "        print(\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§...\")\n",
    "        \n",
    "        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø§Ø±Ø¨Ø±\n",
    "        user_groups = list(preprocessed_data.groupby('user'))\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†: {len(user_groups)}\")\n",
    "        \n",
    "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±\n",
    "        if user_groups:\n",
    "            user_name, user_df = user_groups[0]\n",
    "            print(f\"\\nØ§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±: {user_name}\")\n",
    "            print(f\"ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±: {len(user_df)}\")\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "            user_df_with_features = user_feats_extractor.extract(user_df)\n",
    "            \n",
    "            print(f\"âœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù†Ø¯\")\n",
    "            print(f\"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡:\")\n",
    "            new_columns = [col for col in user_df_with_features.columns if col not in preprocessed_data.columns]\n",
    "            for col in new_columns:\n",
    "                print(f\"   - {col}\")\n",
    "            \n",
    "            # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡\n",
    "            if 'partisan_dist' in user_df_with_features.columns:\n",
    "                sample_dist = user_df_with_features.iloc[0]['partisan_dist']\n",
    "                print(f\"\\nÙ†Ù…ÙˆÙ†Ù‡ ØªÙˆØ²ÛŒØ¹ Ø­Ø²Ø¨ÛŒ: {sample_dist}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc9ae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 6: Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ\n",
      "============================================================\n",
      "âš ï¸ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 6: Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 6: Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if loaded_modules['labelling_functions'] and preprocessed_data is not None:\n",
    "    print(\"Ø§Ø¹Ù…Ø§Ù„ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¨Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\")\n",
    "    \n",
    "    try:\n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ\n",
    "        labelling_data = preprocessed_data.copy()\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ\n",
    "        print(\"\\nØ§ÛŒØ¬Ø§Ø¯ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ...\")\n",
    "        \n",
    "        # ØªØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙˆØ²ÛŒØ¹ Ø­Ø²Ø¨ÛŒ\n",
    "        ps_lf = PS_LF(threshold=0.1, min_count=10)\n",
    "        print(f\"âœ… ØªØ§Ø¨Ø¹ PS_LF Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        \n",
    "        # ØªØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÛŒØªÙˆÛŒÛŒØª Ù…Ø³ØªÙ‚ÛŒÙ…\n",
    "        drt_lf = DRT_LF(threshold=0.15, min_count=25)\n",
    "        print(f\"âœ… ØªØ§Ø¨Ø¹ DRT_LF Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        \n",
    "        # ØªØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ø±Ø¯Ù† Ø³ÛŒØ§Ø³ØªÙ…Ø¯Ø§Ø±Ø§Ù†\n",
    "        if pol_df is not None:\n",
    "            pol_lf = PolFol_LF(pol_df=pol_df, threshold=0.05, min_count=5)\n",
    "            print(f\"âœ… ØªØ§Ø¨Ø¹ PolFol_LF Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        \n",
    "        # ØªØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø³Ø·Ø­ ØªÙˆÛŒÛŒØª\n",
    "        tweet_lf = TweetLevelLF()\n",
    "        print(f\"âœ… ØªØ§Ø¨Ø¹ TweetLevelLF Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        \n",
    "        # Ø§Ø¹Ù…Ø§Ù„ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ (Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆÚ†Ú©)\n",
    "        sample_for_labelling = labelling_data.head(100).copy()\n",
    "        \n",
    "        print(f\"\\nØ§Ø¹Ù…Ø§Ù„ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¨Ø± {len(sample_for_labelling)} Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "        \n",
    "        # Ø§Ø¹Ù…Ø§Ù„ ØªØ§Ø¨Ø¹ PS_LF\n",
    "        sample_for_labelling = ps_lf.label(sample_for_labelling)\n",
    "        print(f\"   - ØªØ§Ø¨Ø¹ PS_LF Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯\")\n",
    "        \n",
    "        # Ø§Ø¹Ù…Ø§Ù„ ØªØ§Ø¨Ø¹ DRT_LF\n",
    "        sample_for_labelling = drt_lf.label(sample_for_labelling)\n",
    "        print(f\"   - ØªØ§Ø¨Ø¹ DRT_LF Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯\")\n",
    "        \n",
    "        # Ø§Ø¹Ù…Ø§Ù„ ØªØ§Ø¨Ø¹ PolFol_LF\n",
    "        if 'pol_lf' in locals():\n",
    "            sample_for_labelling = pol_lf.label(sample_for_labelling)\n",
    "            print(f\"   - ØªØ§Ø¨Ø¹ PolFol_LF Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯\")\n",
    "        \n",
    "        # Ø§Ø¹Ù…Ø§Ù„ ØªØ§Ø¨Ø¹ TweetLevelLF\n",
    "        sample_for_labelling = tweet_lf.label(sample_for_labelling)\n",
    "        print(f\"   - ØªØ§Ø¨Ø¹ TweetLevelLF Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯\")\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡\n",
    "        label_columns = [col for col in sample_for_labelling.columns if 'label' in col]\n",
    "        print(f\"\\nØ³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡: {label_columns}\")\n",
    "        \n",
    "        # Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "        for col in label_columns:\n",
    "            if col in sample_for_labelling.columns:\n",
    "                value_counts = sample_for_labelling[col].value_counts()\n",
    "                print(f\"\\nØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {col}:\")\n",
    "                for value, count in value_counts.items():\n",
    "                    print(f\"   Ø¨Ø±Ú†Ø³Ø¨ {value}: {count} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "        \n",
    "        labelled_data = sample_for_labelling\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}\")\n",
    "        labelled_data = None\n",
    "else:\n",
    "    print(\"âš ï¸ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")\n",
    "    labelled_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c60d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 7: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ø¯Ù‡ Ø´Ø¯Ù‡\n",
      "============================================================\n",
      "âš ï¸ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 7: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ø¯Ù‡ Ø´Ø¯Ù‡\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 7: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ø¯Ù‡ Ø´Ø¯Ù‡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if loaded_modules['analysis_utils'] and labelled_data is not None:\n",
    "    print(\"Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ø¯Ù‡ Ø´Ø¯Ù‡...\")\n",
    "    \n",
    "    try:\n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ Ù¾ÙˆØ´Ø´ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "        print(\"\\n1. Ø¨Ø±Ø±Ø³ÛŒ Ù¾ÙˆØ´Ø´ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§:\")\n",
    "        coverage_df = get_coverage(labelled_data)\n",
    "        print(coverage_df.to_string())\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø± Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ\n",
    "        print(\"\\n2. Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ:\")\n",
    "        get_labelling_stats(labelled_data)\n",
    "        \n",
    "        # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ§ÙÙ‚ Ùˆ ØªØ¹Ø§Ø±Ø¶ Ø¨ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ\n",
    "        if 'unanimous_label' in labelled_data.columns:\n",
    "            print(\"\\n3. ØªØ­Ù„ÛŒÙ„ ØªÙˆØ§ÙÙ‚ Ø¨ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ:\")\n",
    "            \n",
    "            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø±ØµØ¯ ØªÙˆØ§ÙÙ‚\n",
    "            agreement_stats = labelled_data['unanimous_label'].value_counts(normalize=True)\n",
    "            print(\"ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ØªÙˆØ§ÙÙ‚ÛŒ:\")\n",
    "            for label, percent in agreement_stats.items():\n",
    "                print(f\"   Ø¨Ø±Ú†Ø³Ø¨ {label}: {percent*100:.1f}%\")\n",
    "            \n",
    "            # Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø§Ø±Ø¶Ø§Øª\n",
    "            conflicts = (labelled_data['unanimous_label'] == -100).sum()\n",
    "            total = len(labelled_data)\n",
    "            print(f\"\\nØªØ¹Ø¯Ø§Ø¯ ØªØ¹Ø§Ø±Ø¶Ø§Øª: {conflicts} Ø§Ø² {total} ({conflicts/total*100:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a03a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 8: Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ PyTorch Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
      "============================================================\n",
      "âš ï¸ Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 8: Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ PyTorch\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 8: Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ PyTorch Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if loaded_modules['torch_datasets'] and labelled_data is not None and news_df is not None:\n",
    "    print(\"Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ PyTorch Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ø¯Ù‡ Ø´Ø¯Ù‡...\")\n",
    "    \n",
    "    try:\n",
    "        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª\n",
    "        # Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø³ØªÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ø¯Ø§Ø±ÛŒÙ…\n",
    "        if 'keyword_label' in labelled_data.columns:\n",
    "            label_col = 'keyword_label'\n",
    "        elif 'user_label' in labelled_data.columns:\n",
    "            label_col = 'user_label'\n",
    "        else:\n",
    "            # Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ù†Ù…ÙˆÙ†Ù‡\n",
    "            print(\"Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "            labelled_data['sample_label'] = np.random.choice([0, 1], len(labelled_data))\n",
    "            label_col = 'sample_label'\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª Ù…ØªÙ†\n",
    "        print(f\"\\n1. Ø§ÛŒØ¬Ø§Ø¯ TextDataset Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ {label_col}...\")\n",
    "        text_dataset = TextDataset(\n",
    "            dataset_df=labelled_data,\n",
    "            label_col=label_col,\n",
    "            pre_trained=\"cardiff\",\n",
    "            max_length=50\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… TextDataset Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        print(f\"   - ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(text_dataset)}\")\n",
    "        \n",
    "        # ØªØ³Øª ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡\n",
    "        sample = text_dataset[0]\n",
    "        print(f\"   - Ø´Ú©Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†: {sample['feats'].shape}\")\n",
    "        print(f\"   - Ø¨Ø±Ú†Ø³Ø¨ Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„: {sample['label'].item()}\")\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø± (Ø§Ú¯Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)\n",
    "        print(f\"\\n2. Ø§ÛŒØ¬Ø§Ø¯ UserDataset...\")\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ pol_df Ù†Ù…ÙˆÙ†Ù‡ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯\n",
    "        if 'pol_df' not in locals():\n",
    "            pol_df = pd.DataFrame({\n",
    "                'user_name': ['sample_pol1', 'sample_pol2'],\n",
    "                'party_name': ['Republican', 'Democrat']\n",
    "            })\n",
    "        \n",
    "        user_dataset = UserDataset(\n",
    "            dataset_df=labelled_data,\n",
    "            news_df=news_df,\n",
    "            pol_df=pol_df,\n",
    "            label_col=label_col\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… UserDataset Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        print(f\"   - ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(user_dataset)}\")\n",
    "        \n",
    "        # ØªØ³Øª ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡\n",
    "        user_sample = user_dataset[0]\n",
    "        print(f\"   - Ø´Ú©Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±: {user_sample['feats'].shape}\")\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ DataLoader Ø¨Ø±Ø§ÛŒ ØªØ³Øª\n",
    "        print(f\"\\n3. Ø§ÛŒØ¬Ø§Ø¯ DataLoader...\")\n",
    "        batch_size = 32\n",
    "        text_dataloader = DataLoader(\n",
    "            text_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… DataLoader Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        print(f\"   - Ø§Ù†Ø¯Ø§Ø²Ù‡ batch: {batch_size}\")\n",
    "        print(f\"   - ØªØ¹Ø¯Ø§Ø¯ batchÙ‡Ø§: {len(text_dataloader)}\")\n",
    "        \n",
    "        # ØªØ³Øª ÛŒÚ© batch\n",
    "        for batch in text_dataloader:\n",
    "            print(f\"   - Ø´Ú©Ù„ batch ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {batch['feats'].shape}\")\n",
    "            print(f\"   - Ø´Ú©Ù„ batch Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§: {batch['label'].shape}\")\n",
    "            break\n",
    "        \n",
    "        datasets_created = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§: {e}\")\n",
    "        datasets_created = False\n",
    "else:\n",
    "    print(\"âš ï¸ Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")\n",
    "    datasets_created = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4afce728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 9: Ø§ÛŒØ¬Ø§Ø¯ Ùˆ ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ\n",
      "============================================================\n",
      "âš ï¸ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 9: Ø§ÛŒØ¬Ø§Ø¯ Ùˆ ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 9: Ø§ÛŒØ¬Ø§Ø¯ Ùˆ ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if loaded_modules['models'] and datasets_created:\n",
    "    print(\"Ø§ÛŒØ¬Ø§Ø¯ Ùˆ ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ...\")\n",
    "    \n",
    "    try:\n",
    "        # 9.1 Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ù…ØªÙ†\n",
    "        print(\"\\n1. Ø§ÛŒØ¬Ø§Ø¯ TextNetwork...\")\n",
    "        text_model = TextNetwork(\n",
    "            input_shape=50,\n",
    "            lh=128,\n",
    "            freeze_bert=True,\n",
    "            dropout=0.1,\n",
    "            pre_trained=\"cardiff\",\n",
    "            pooling=\"mean\"\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… TextNetwork Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        print(f\"   - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„: {text_model.params_}\")\n",
    "        \n",
    "        # 9.2 Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ú©Ø§Ø±Ø¨Ø±\n",
    "        print(\"\\n2. Ø§ÛŒØ¬Ø§Ø¯ UserNetwork...\")\n",
    "        \n",
    "        # ØªØ¹ÛŒÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±\n",
    "        if 'user_dataset' in locals():\n",
    "            input_shape = user_dataset[0]['feats'].shape[0]\n",
    "        else:\n",
    "            input_shape = 100  # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶\n",
    "        \n",
    "        user_model = UserNetwork(\n",
    "            input_shape=input_shape,\n",
    "            l1_h=512,\n",
    "            dropout=0.1,\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… UserNetwork Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        print(f\"   - Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ±ÙˆØ¯ÛŒ: {input_shape}\")\n",
    "        print(f\"   - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„: {user_model.params_}\")\n",
    "        \n",
    "        # 9.3 Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ ØªØ±Ú©ÛŒØ¨ÛŒ\n",
    "        print(\"\\n3. Ø§ÛŒØ¬Ø§Ø¯ CombinedNetwork...\")\n",
    "        combined_model = CombinedNetwork(\n",
    "            user_input_shape=input_shape,\n",
    "            text_lh=128,\n",
    "            user_lh=64,\n",
    "            freeze_bert=True,\n",
    "            dropout=0.1,\n",
    "            pre_trained=\"cardiff\",\n",
    "            pooling=\"mean\",\n",
    "            user_activation=\"relu\",\n",
    "            text_activation=\"relu\"\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… CombinedNetwork Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        print(f\"   - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„: {combined_model.params_}\")\n",
    "        \n",
    "        # 9.4 ØªØ³Øª forward pass\n",
    "        print(\"\\n4. ØªØ³Øª forward pass Ù…Ø¯Ù„â€ŒÙ‡Ø§...\")\n",
    "        \n",
    "        # ØªØ³Øª TextNetwork\n",
    "        if 'text_dataloader' in locals():\n",
    "            for batch in text_dataloader:\n",
    "                test_input = batch['feats']\n",
    "                print(f\"   - ÙˆØ±ÙˆØ¯ÛŒ ØªØ³Øª TextNetwork: {test_input.shape}\")\n",
    "                \n",
    "                # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ù…Ù†Ø§Ø³Ø¨\n",
    "                test_input = test_input.to(device)\n",
    "                text_model = text_model.to(device)\n",
    "                \n",
    "                # forward pass\n",
    "                with torch.no_grad():\n",
    "                    hidden, output = text_model(test_input)\n",
    "                    print(f\"   - Ø®Ø±ÙˆØ¬ÛŒ TextNetwork: {output.shape}\")\n",
    "                    print(f\"   - hidden layer: {hidden.shape}\")\n",
    "                break\n",
    "        \n",
    "        models_created = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§: {e}\")\n",
    "        models_created = False\n",
    "else:\n",
    "    print(\"âš ï¸ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")\n",
    "    models_created = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdf87e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 10: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
      "============================================================\n",
      "âš ï¸ ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 10: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 10: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if labelled_data is not None:\n",
    "    print(\"ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ train Ùˆ validation...\")\n",
    "    \n",
    "    try:\n",
    "        # ØªØ¹ÛŒÛŒÙ† Ø³ØªÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨\n",
    "        if 'keyword_label' in labelled_data.columns:\n",
    "            label_col = 'keyword_label'\n",
    "        elif 'user_label' in labelled_data.columns:\n",
    "            label_col = 'user_label'\n",
    "        elif 'sample_label' in labelled_data.columns:\n",
    "            label_col = 'sample_label'\n",
    "        else:\n",
    "            # Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
    "            labelled_data['train_label'] = np.random.choice([0, 1], len(labelled_data))\n",
    "            label_col = 'train_label'\n",
    "        \n",
    "        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨\n",
    "        valid_data = labelled_data[labelled_data[label_col] != -1].copy()\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            print(f\"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: {len(valid_data)}\")\n",
    "            \n",
    "            # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "            train_df, val_df = train_test_split(\n",
    "                valid_data,\n",
    "                test_size=0.2,\n",
    "                random_state=42,\n",
    "                stratify=valid_data[label_col] if len(valid_data[label_col].unique()) > 1 else None\n",
    "            )\n",
    "            \n",
    "            print(f\"   - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: {len(train_df)} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "            print(f\"   - Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {len(val_df)} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "            \n",
    "            # Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
    "            print(f\"\\nØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´:\")\n",
    "            train_counts = train_df[label_col].value_counts()\n",
    "            for label, count in train_counts.items():\n",
    "                print(f\"   Ø¨Ø±Ú†Ø³Ø¨ {label}: {count} ({count/len(train_df)*100:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ:\")\n",
    "            val_counts = val_df[label_col].value_counts()\n",
    "            for label, count in val_counts.items():\n",
    "                print(f\"   Ø¨Ø±Ú†Ø³Ø¨ {label}: {count} ({count/len(val_df)*100:.1f}%)\")\n",
    "            \n",
    "            data_split = True\n",
    "            \n",
    "        else:\n",
    "            print(\"âš ï¸ Ù‡ÛŒÚ† Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ Ù…Ù†Ø§Ø³Ø¨ ÛŒØ§ÙØª Ù†Ø´Ø¯\")\n",
    "            data_split = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}\")\n",
    "        data_split = False\n",
    "else:\n",
    "    print(\"âš ï¸ ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")\n",
    "    data_split = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a02d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 11: Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ (Ø¢Ù…ÙˆØ²Ø´ Ú©ÙˆØªØ§Ù‡)\n",
      "============================================================\n",
      "âš ï¸ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 11: Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ (Ø¢Ù…ÙˆØ²Ø´ Ú©ÙˆØªØ§Ù‡)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 11: Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ (Ø¢Ù…ÙˆØ²Ø´ Ú©ÙˆØªØ§Ù‡)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if (loaded_modules['model_utils'] and loaded_modules['torch_datasets'] and \n",
    "    data_split and 'train_df' in locals() and 'val_df' in locals()):\n",
    "    \n",
    "    print(\"Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ú©ÙˆØªØ§Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙØ±Ø¢ÛŒÙ†Ø¯...\")\n",
    "    \n",
    "    try:\n",
    "        # Ø§Ù†ØªØ®Ø§Ø¨ Ø²ÛŒØ±Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø³Ø±ÛŒØ¹\n",
    "        sample_train_size = min(200, len(train_df))\n",
    "        sample_val_size = min(50, len(val_df))\n",
    "        \n",
    "        sample_train = train_df.head(sample_train_size).copy()\n",
    "        sample_val = val_df.head(sample_val_size).copy()\n",
    "        \n",
    "        print(f\"Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ú©ÙˆØªØ§Ù‡:\")\n",
    "        print(f\"   - Ø¢Ù…ÙˆØ²Ø´: {sample_train_size} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "        print(f\"   - Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {sample_val_size} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "        \n",
    "        # ØªØ¹ÛŒÛŒÙ† Ø³ØªÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨\n",
    "        if 'keyword_label' in sample_train.columns:\n",
    "            label_col = 'keyword_label'\n",
    "        elif 'user_label' in sample_train.columns:\n",
    "            label_col = 'user_label'\n",
    "        else:\n",
    "            label_col = 'sample_label' if 'sample_label' in sample_train.columns else 'train_label'\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§\n",
    "        print(\"\\nØ§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª Ù…ØªÙ†\n",
    "        train_text_dataset = TextDataset(\n",
    "            dataset_df=sample_train,\n",
    "            label_col=label_col,\n",
    "            pre_trained=\"cardiff\",\n",
    "            max_length=50\n",
    "        )\n",
    "        \n",
    "        val_text_dataset = TextDataset(\n",
    "            dataset_df=sample_val,\n",
    "            label_col=label_col,\n",
    "            pre_trained=\"cardiff\",\n",
    "            max_length=50\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ† Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯\")\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„\n",
    "        print(\"\\nØ§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "        sample_model = TextNetwork(\n",
    "            input_shape=50,\n",
    "            lh=64,  # Ú©ÙˆÚ†Ú©ØªØ± Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø³Ø±ÛŒØ¹\n",
    "            freeze_bert=True,\n",
    "            dropout=0.1,\n",
    "            pre_trained=\"cardiff\",\n",
    "            pooling=\"mean\"\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Ù…Ø¯Ù„ Ù†Ù…ÙˆÙ†Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯\")\n",
    "        \n",
    "        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
    "        lr = 0.001\n",
    "        epochs = 3  # ÙÙ‚Ø· 3 Ø¯ÙˆØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
    "        batch_size = 16\n",
    "        \n",
    "        print(f\"\\nÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù†Ù…ÙˆÙ†Ù‡:\")\n",
    "        print(f\"   - Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ: {lr}\")\n",
    "        print(f\"   - ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§: {epochs}\")\n",
    "        print(f\"   - Ø§Ù†Ø¯Ø§Ø²Ù‡ batch: {batch_size}\")\n",
    "        \n",
    "        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ (Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡ - Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¢Ù† Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯)\n",
    "        print(\"\\nâš ï¸ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡ Ø§Ø³Øª (Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ)\")\n",
    "        print(\"Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ú©Ø§Ù…Ù†Øª Ø®Ø· Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯:\")\n",
    "        print(\"\"\"\n",
    "        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
    "        best_model, train_losses, val_losses, train_accs, val_accs = train_network(\n",
    "            train_dataset=train_text_dataset,\n",
    "            val_dataset=val_text_dataset,\n",
    "            model=sample_model,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            seed=42,\n",
    "            plot=True,\n",
    "            patience=2,\n",
    "            user=False,\n",
    "            device_ids=[0] if torch.cuda.is_available() else []\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Ø¨Ù‡ Ø¬Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ ÙˆØ§Ù‚Ø¹ÛŒØŒ ÛŒÚ© Ø³Ø§Ø®ØªØ§Ø± Ù†Ù…ÙˆÙ†Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…\n",
    "        print(\"\\nØ³Ø§Ø®ØªØ§Ø± Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ù…ÙˆØ²Ø´:\")\n",
    "        print(\"\"\"\n",
    "        best_model: Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ†\n",
    "        train_losses: Ù„ÛŒØ³Øª lossÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± Ù‡Ø± Ø¯ÙˆØ±Ù‡\n",
    "        val_losses: Ù„ÛŒØ³Øª lossÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø± Ù‡Ø± Ø¯ÙˆØ±Ù‡  \n",
    "        train_accs: Ù„ÛŒØ³Øª Ø¯Ù‚Øªâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± Ù‡Ø± Ø¯ÙˆØ±Ù‡\n",
    "        val_accs: Ù„ÛŒØ³Øª Ø¯Ù‚Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø± Ù‡Ø± Ø¯ÙˆØ±Ù‡\n",
    "        \"\"\")\n",
    "        \n",
    "        training_demo = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´: {e}\")\n",
    "        training_demo = False\n",
    "else:\n",
    "    print(\"âš ï¸ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")\n",
    "    training_demo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a98e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 12: ØªØ¬Ø³Ù… Ùˆ ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬\n",
      "============================================================\n",
      "âš ï¸ Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù… Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 12: ØªØ¬Ø³Ù… Ùˆ ØªØ­Ù„ÛŒÙ„ (Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 12: ØªØ¬Ø³Ù… Ùˆ ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if loaded_modules['plot_utils'] and labelled_data is not None:\n",
    "    print(\"Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù…...\")\n",
    "    \n",
    "    try:\n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¬Ø³Ù…\n",
    "        print(\"\\nØ§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù…...\")\n",
    "        \n",
    "        # Ø¨Ø±Ø§ÛŒ plot_distrust_ratios_custom Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµÛŒ Ø¯Ø§Ø±ÛŒÙ…\n",
    "        viz_data = labelled_data.copy()\n",
    "        \n",
    "        # Ø§Ú¯Ø± Ø³ØªÙˆÙ† model_preds ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
    "        if 'model_preds' not in viz_data.columns:\n",
    "            print(\"Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¬Ø³Ù…...\")\n",
    "            viz_data['model_preds'] = np.random.choice([0, 1], len(viz_data))\n",
    "            viz_data['avg_stance'] = np.random.uniform(-3, 3, len(viz_data))\n",
    "            viz_data['direct_retweet'] = np.random.choice([True, False], len(viz_data))\n",
    "        \n",
    "        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù… (Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡)\n",
    "        print(\"\\nØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù… Ù…ÙˆØ¬ÙˆØ¯:\")\n",
    "        print(\"1. plot_distrust_ratios_custom(): Ø±Ø³Ù… Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ù… Ø§Ø¹ØªÙ…Ø§Ø¯\")\n",
    "        print(\"2. plot_stance_entropy(): Ø±Ø³Ù… Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ù…ÙˆØ¶Ø¹\")\n",
    "        print(\"3. plot_roc_curves(): Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ ROC\")\n",
    "        print(\"4. criticism_by_time(): ØªØ­Ù„ÛŒÙ„ Ø§Ù†ØªÙ‚Ø§Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†\")\n",
    "        \n",
    "        print(\"\\nâš ï¸ ØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¯Ø§Ø±Ù†Ø¯)\")\n",
    "        print(\"Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†Ù‡Ø§ØŒ Ú©Ø§Ù…Ù†Øª Ø®Ø·ÙˆØ· Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯:\")\n",
    "        print(\"\"\"\n",
    "        # Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² plot_stance_entropy\n",
    "        # plot_stance_entropy(viz_data)\n",
    "        \n",
    "        # Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² plot_distrust_ratios_custom  \n",
    "        # plot_distrust_ratios_custom(viz_data, bin_ranges=[(-3,-2),(-2,-1),(-1,1),(1,2),(2,3)])\n",
    "        \"\"\")\n",
    "        \n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´\n",
    "        print(\"\\nØ§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù†Ù…ÙˆØ¯Ø§Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø³Ø§Ø¯Ù‡...\")\n",
    "        \n",
    "        if 'matched_partisans' in viz_data.columns:\n",
    "            # Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØªÙˆØ²ÛŒØ¹ Ù…ÙˆØ¶Ø¹â€ŒÙ‡Ø§ÛŒ Ø­Ø²Ø¨ÛŒ\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(viz_data['matched_partisans'], bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            plt.xlabel('Partisan Score', fontsize=12)\n",
    "            plt.ylabel('Frequency', fontsize=12)\n",
    "            plt.title('Distribution of Partisan Scores in Sample Data', fontsize=14)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±\n",
    "            plot_path = os.path.join(current_dir, 'sample_plot.png')\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"âœ… Ù†Ù…ÙˆØ¯Ø§Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± {plot_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯\")\n",
    "        \n",
    "        visualization_demo = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ ØªØ¬Ø³Ù…: {e}\")\n",
    "        visualization_demo = False\n",
    "else:\n",
    "    print(\"âš ï¸ Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù… Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª\")\n",
    "    visualization_demo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57ca782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ø¨Ø®Ø´ 13: Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§\n",
      "============================================================\n",
      "\n",
      "Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒ notebook:\n",
      "=====================\n",
      "\n",
      "âœ… Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Ø¨Ø®Ø´ 13: Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Ø®Ù„Ø§ØµÙ‡\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ø¨Ø®Ø´ 13: Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒ notebook:\n",
    "=====================\n",
    "\n",
    "âœ… Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡:\n",
    "\"\"\")\n",
    "\n",
    "# Ù„ÛŒØ³Øª Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚\n",
    "success_sections = []\n",
    "\n",
    "if news_df is not None:\n",
    "    success_sections.append(\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ\")\n",
    "if annotated_df is not None:\n",
    "    success_sections.append(\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø§Ø´ÛŒÙ‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡\")\n",
    "if analysis_df is not None:\n",
    "    success_sections.append(\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„\")\n",
    "\n",
    "# Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡\n",
    "for module_name, status in loaded_modules.items():\n",
    "    if status:\n",
    "        success_sections.append(f\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {module_name}\")\n",
    "\n",
    "if preprocessed_data is not None:\n",
    "    success_sections.append(\"Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\")\n",
    "if labelled_data is not None:\n",
    "    success_sections.append(\"Ø¨Ø±Ú†Ø³Ø¨â€ŒØ²Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\")\n",
    "if datasets_created:\n",
    "    success_sections.append(\"Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ PyTorch\")\n",
    "if models_created:\n",
    "    success_sections.append(\"Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ\")\n",
    "if data_split:\n",
    "    success_sections.append(\"ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\")\n",
    "if visualization_demo:\n",
    "    success_sections.append(\"Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ§Ø¨Ø¹ ØªØ¬Ø³Ù…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c73ce8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ feature_extractors\n",
      "   2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ analysis_utils\n",
      "\n",
      "ğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ø§Ø¬Ø±Ø§:\n",
      "   - ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¨Ø®Ø´â€ŒÙ‡Ø§: 13\n",
      "   - Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡: 2\n",
      "   - Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: 15.4%\n",
      "\n",
      "ğŸ“‹ Ú¯Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡:\n",
      "======================================\n",
      "\n",
      "1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„:\n",
      "   - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± data_for_analysis_anonymized.jsonl\n",
      "   - ØªØ·Ø¨ÛŒÙ‚ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ù†ØªØ¸Ø§Ø±Ø§Øª Ú©Ø¯\n",
      "\n",
      "2. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§:\n",
      "   - Ø¢Ù…ÙˆØ²Ø´ TextNetwork Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†\n",
      "   - Ø¢Ù…ÙˆØ²Ø´ UserNetwork Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±\n",
      "   - Ø¢Ù…ÙˆØ²Ø´ CombinedNetwork Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ù‡Ø± Ø¯Ùˆ\n",
      "\n",
      "3. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§:\n",
      "   - Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ø¯Ù‚ØªØŒ recallØŒ F1-scoreØŒ AUC)\n",
      "   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù\n",
      "\n",
      "4. ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬:\n",
      "   - ØªØ¬Ø³Ù… ØªÙˆØ²ÛŒØ¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§\n",
      "   - ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù…Ø¯Ù„\n",
      "   - ØªÙØ³ÛŒØ± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…\n",
      "\n",
      "5. Ø§Ø³ØªÙ†ØªØ§Ø¬ Ùˆ deployment:\n",
      "   - Ø§ÛŒØ¬Ø§Ø¯ Ø®Ø· Ù„ÙˆÙ„Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯\n",
      "   - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ\n",
      "\n",
      "ğŸ”§ Ù†Ú©Ø§Øª ÙÙ†ÛŒ:\n",
      "===========\n",
      "\n",
      "- Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ù†ÛŒØ§Ø² Ø¨Ù‡ GPU Ø¨Ø§ Ø­Ø§ÙØ¸Ù‡ Ú©Ø§ÙÛŒ Ø¯Ø§Ø±ÛŒØ¯\n",
      "- Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ BERT-based Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§Ø´Ø¯\n",
      "- Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ø¯Ø§Ø±Ø¯\n",
      "\n",
      "ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø§Ø¬Ø±Ø§:\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ù†Ù…Ø§ÛŒØ´ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚\n",
    "for i, section in enumerate(success_sections, 1):\n",
    "    print(f\"   {i}. {section}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ø§Ø¬Ø±Ø§:\")\n",
    "print(f\"   - ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¨Ø®Ø´â€ŒÙ‡Ø§: 13\")\n",
    "print(f\"   - Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡: {len(success_sections)}\")\n",
    "print(f\"   - Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: {len(success_sections)/13*100:.1f}%\")\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“‹ Ú¯Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡:\n",
    "======================================\n",
    "\n",
    "1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„:\n",
    "   - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± data_for_analysis_anonymized.jsonl\n",
    "   - ØªØ·Ø¨ÛŒÙ‚ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ù†ØªØ¸Ø§Ø±Ø§Øª Ú©Ø¯\n",
    "\n",
    "2. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§:\n",
    "   - Ø¢Ù…ÙˆØ²Ø´ TextNetwork Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†\n",
    "   - Ø¢Ù…ÙˆØ²Ø´ UserNetwork Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±\n",
    "   - Ø¢Ù…ÙˆØ²Ø´ CombinedNetwork Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ù‡Ø± Ø¯Ùˆ\n",
    "\n",
    "3. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§:\n",
    "   - Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ø¯Ù‚ØªØŒ recallØŒ F1-scoreØŒ AUC)\n",
    "   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù\n",
    "\n",
    "4. ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬:\n",
    "   - ØªØ¬Ø³Ù… ØªÙˆØ²ÛŒØ¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§\n",
    "   - ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù…Ø¯Ù„\n",
    "   - ØªÙØ³ÛŒØ± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…\n",
    "\n",
    "5. Ø§Ø³ØªÙ†ØªØ§Ø¬ Ùˆ deployment:\n",
    "   - Ø§ÛŒØ¬Ø§Ø¯ Ø®Ø· Ù„ÙˆÙ„Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯\n",
    "   - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ\n",
    "\n",
    "ğŸ”§ Ù†Ú©Ø§Øª ÙÙ†ÛŒ:\n",
    "===========\n",
    "\n",
    "- Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ù†ÛŒØ§Ø² Ø¨Ù‡ GPU Ø¨Ø§ Ø­Ø§ÙØ¸Ù‡ Ú©Ø§ÙÛŒ Ø¯Ø§Ø±ÛŒØ¯\n",
    "- Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ BERT-based Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§Ø´Ø¯\n",
    "- Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ø¯Ø§Ø±Ø¯\n",
    "\n",
    "ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø§Ø¬Ø±Ø§:\n",
    "===============================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb543bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ‰ Ø§Ø¬Ø±Ø§ÛŒ notebook Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\n",
      "============================================================\n",
      "\n",
      "Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡:\n",
      "1. Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯\n",
      "2. Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯\n",
      "3. Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯\n",
      "\n",
      "Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´ÛŒØ¯! âœ¨\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡\n",
    "created_files = []\n",
    "if 'preprocessed_path' in locals() and os.path.exists(preprocessed_path):\n",
    "    created_files.append(preprocessed_path)\n",
    "if 'plot_path' in locals() and os.path.exists(plot_path):\n",
    "    created_files.append(plot_path)\n",
    "\n",
    "for file in created_files:\n",
    "    file_size = os.path.getsize(file) / 1024  # Ø­Ø¬Ù… Ø¨Ù‡ Ú©ÛŒÙ„ÙˆØ¨Ø§ÛŒØª\n",
    "    print(f\"   - {os.path.basename(file)} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ Ø§Ø¬Ø±Ø§ÛŒ notebook Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ù†Ù‡Ø§ÛŒÛŒ\n",
    "print(\"\"\"\n",
    "Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡:\n",
    "1. Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯\n",
    "2. Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯\n",
    "3. Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯\n",
    "\n",
    "Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´ÛŒØ¯! âœ¨\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba7938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
